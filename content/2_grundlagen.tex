\chapter{Technische Grundlagen}
\section{\acf{OLAP}}
Viele Unternehmen nutzen heute große Mengen von Geschäftsdaten zur strategischen Planung.
Durch eine Analyse von Geschäftsdaten lassen sich Handlungsempfehlungen ableiten.
Typische Analysen sind etwa die Berechnung des Umsatzes einer Filiale innerhalb des letzten Jahres und der Vergleich zu vorherigen Jahren oder die Analyse der Verkäufe von einzelnen Produkten in einem bestimmten Quartal.
Diese Analysen erfordern eine große Datenmenge, die auch historische Daten beinhaltet.
Klassische Datenbanksysteme für Transaktionen, etwa in einem Onlineshop, auch \acf{OLTP} genannt, sind für viele Schreib- und Lesezugriffe optimiert, nutzen meist wenig komplexe Abfragen (Queries) und enthalten häufig keine historischen Daten.
Datenbanken für analytische Zwecke sollten hingegen stark auf die Leseoperationen optimiert sein und müssen komplexere Queries in angemessener Zeit durchführen können.
Bei diesem Einsatzgebiet von Datenbanken spricht man vom \acf{OLAP}~\cite[S.~97f]{kleppmann_datenintensive_2019}.

\ac{OLAP} und \ac{OLTP} profitieren von verschiedenen Optimierungen der Datenstrukturen in einer Datenbank.
Bei Nutzung einer gemeinsamen Datenbank für beide Verfahren würden die \ac{OLAP}-Abfragen außerdem die \ac{OLTP}-Transaktionen verlangsamen oder blockieren.
Um das zu verhindern, nutzt man für \ac{OLAP} eigene Datenbanksysteme und speichert die Daten in sogenannten Data Warehouses.

%Hier richtig zitieren sobald Seitenzahl von Kleppmann bekannt S97f
\subsection{Data Warehouses}
Data Warehouses wurden entwickelt, um Daten, die für strategische Entscheidungen im Geschäftsumfeld nützlich sein können, zu speichern.
Dazu werden Daten aus Geschäftsprozessen wie etwa Verkäufen extrahiert, für die weitere Verwendung transformiert und in bestimmten Datenbanken gespeichert.
Klassische  operative oder transaktionale Datenbankansätze, die auf den täglichen Gebrauch durch viele Nutzende optimiert sind, eignen sich nicht für ein Data Warehouse im \ac{OLAP}-Umfeld, da hier üblicherweise keine historischen Daten gespeichert werden.
Des Weiteren enthalten diese detaillierte Daten was das Ausführen von komplexen Abfragen erschwert bzw. verlangsamt.
% Hier ähnliche Inhalte wie im Kapitel weiter oben
Um aber Entscheidungen aus den Daten ableiten zu können, sind sowohl historische Daten als auch komplexe Abfragen notwendig.
Um diese Probleme zu lösen, enthält ein Data Warehouse im \ac{OLAP} Umfeld historische Daten und bietet eine optimierte Datenstruktur für komplexere Abfragen.
In der ursprünglichen Definition nach Inmon werden Data Warehouses als Sammlung von subjektorientierten, integrierten, nicht flüchtigen und zeitvariable Daten beschrieben. %Hier wäre Originalquelle ganz praktisch
Subjektorientiert bedeutet, dass sich die Daten auf bestimmte Aspekte der geforderten Analyse beziehen, also z.~B. Daten über Produktionsmengen und Verkäufen bei Produktionsunternehmen.
Mit integrierten Daten ist gemeint, dass Daten aus verschiedenen Umgebungen in dem Warehouse integriert sind.
Nicht flüchtig bedeutet, dass die Daten über lange Zeiträume hinweg gespeichert bleiben und somit weder gelöscht noch modifiziert werden.
Zur Analyse ist es wichtig, Daten im zeitlichen Verlauf miteinander zu vergleichen, etwa, um herauszufinden, wie sich die Verkäufe im letzten Quartal entwickelt haben.
Zeitvariabel beschriebt daher, dass Daten aus verschiedenen Zeitpunkten gespeichert werden~\cite[s. 3-4]{vaisman_data_2022}.

\subsection{Datenstruktur in Data Warehouses im \ac{OLAP} Umfeld}
Die Mehrheit der am häufigsten verwendeten Datenbanken nutzt ein relationales Datenbankmodell~\cite{db-engines_most_2023}. 
Eine gängige Praxis bei relationalen Datenbanken ist die Normalisierung.
Ziel der Normalisierung ist das Verhindern von redundanten Daten.
Enthalten mehrere Tabellen die gleichen Daten, so müssen diese Daten an mehreren Stellen bei Änderungen angepasst werden.

Um diesen Umstand zu verhindern, werden die Daten daher nur in einer Tabelle gespeichert und dann per Fremdschlüssel in den anderen Tabellen eingebunden~\cite[S.~24f]{vaisman_data_2022}.
Diese Fremdschlüssel benötigen in den meisten Fällen außerdem weniger Speicherplatz als die eigentlichen Daten, was einen weiteren Vorteil bietet.
Ein Nachteil der Normalisierung ist jedoch, dass Abfragen komplizierter werden und länger in der Ausführung benötigen, da die Daten aus verschiedenen Tabellen kombiniert werden müssen.
In Data Warehouses, die für große Datenmengen und komplexe Abfragen gedacht sind, bildet das Normalisieren durch die erschwerten Abfragen einen Nachteil.
Um Daten für Abfragen effizient zu speichern, werden diese nicht so weit wie möglich normalisiert, sondern anhand ihrer Inhalte verteilt. 
Durch Verständnis der Daten können diese in eine sogenannte Faktentabelle und viele Dimensionstabellen aufgeteilt werden. In der Faktentabelle befinden sich die zentralen Metriken, die zur Analyse benötigt werden, etwa die Anzahl der Verkäufe eines Produktes.
Die Dimensionstabellen enthalten Daten, mit denen die Daten der Faktentabelle unter verschiedenen Umständen betrachtet werden können, etwa eine Dimensionstabelle mit Zeitpunkten oder Orten.
Die Dimensionstabellen sind in der Faktentabelle verlinkt.
Bei Abfragen können nun verschiedene Dimensionen kombiniert werden, um Daten aus der Faktentabelle abzufragen.
So können z.~B. die Verkäufe aus einem bestimmten Jahr in einer bestimmten Filiale aus der Faktentabelle abgefragt werden.
Die Dimensionstabellen können außerdem sogenannte Hierarchien enthalten, die verschiedene Genauigkeiten der Dimension definieren.
So kann ein Zeitpunkt in der Tabelle Werte für Uhrzeit, Tag, Monat, Quartal oder Jahr enthalten.
Ein Ort kann etwa aus dem Ortsnamen, dem Bundesland, dem Land, dem Kontinent bestehen.
So lassen sich genaue Abfragen zu Verkäufen einer bestimmen Filiale oder etwa allen Filialen in Europa abfragen~\cite[s. 5]{vaisman_data_2022}.

% Tabelle überdenken
\begin{table}[ht] 
    \centering
    \footnotesize
    \begin{tabular}{ccccc}
        \toprule  
        Id & Umsatz & Stückzahl & Zeit & Ort \\
        \midrule
        1 & 100,00 € & 50 & 1 & 1 \\
        2 & 200,00 € & 100 & 2 & 2 \\
        3 & 150,00 € & 75 & 3 & 3 \\
        \bottomrule
    \end{tabular}
    \caption{Faktentabelle mit Umsatz, Stückzahl, Zeit und Ort}
    \label{tab:faktentabelle}
\end{table}

\begin{table}[ht] 
    \centering
    \footnotesize
    \begin{tabular}{cccccc}
        \toprule  
        Id & Ortsname & Postleitzahl & Landkreis & Bundesland & Land \\
        \midrule
        1 & Furtwangen & 78120 & Schwarzwald-Baar-Kreis & Baden-Württemberg & Deutschland \\
        2 & Schömberg & 75328 & Landkreis Calw & Baden-Württemberg & Deutschland \\
        3 & Pforzheim & 75173 & Enzkreis & Baden-Württemberg & Deutschland \\
        \bottomrule
    \end{tabular}
    \caption{Ortstabelle mit Ortsname, Postleitzahl, Landkreis, Bundesland und Land}
    \label{tab:ortstabelle}
\end{table}

\begin{table}[ht] 
    \centering
    \footnotesize
    \begin{tabular}{cccccc}
        \toprule  
        Id & Timestamp & Datum & Monat & Quartal & Jahr \\
        \midrule
        1 & 02.12.1998 12:00 & 02.12.1998 & Dezember & 4 & 1998 \\
        2 & 28.12.2007 12:00 & 02.01.2007 & Januar & 1 & 2007 \\
        3 & 03.01.2023 16:00 & 03.01.2023 & Januar & 1 & 2023 \\
        \bottomrule
    \end{tabular}
    \caption{Zeittabelle mit Timestamp, Datum, Monat, Quartal und Jahr}
    \label{tab:zeittabelle}
\end{table}


% Eventuell Kapitel über Query Languages schreiben
\subsection{Die \acs{SQL}-Sprache} % Titel verbessern
% Code Listing ist auf englisch, zu deutsch ändern
\begin{lstlisting}[language=SQL, caption=SQL Befehle zum Anlegen der Tabellen, label=code:sqlcreationoftables]
CREATE TABLE Ortstabelle (
    Id INT PRIMARY KEY,
    Ortsname VARCHAR(255),
    Postleitzahl CHAR(5),
    Landkreis VARCHAR(255),
    Bundesland VARCHAR(255),
    Land VARCHAR(255)
);

CREATE TABLE Zeittabelle (
    Id INT PRIMARY KEY,
    Timestamp TIMESTAMP,
    Datum DATE,
    Monat VARCHAR(255),
    Quartal SMALLINT,
    Jahr SMALLINT
);

CREATE TABLE Faktentabelle (
    Id INT PRIMARY KEY,
    Umsatz VARCHAR(255),
    Stueckzahl INT,
    Zeit INT,
    Ort INT,
    FOREIGN KEY (Zeit) REFERENCES Zeittabelle(Id),
    FOREIGN KEY (Ort) REFERENCES Ortstabelle(Id)
);

\end{lstlisting}

In \cref{code:sqlcreationoftables} werden \acs{SQL}-Befehle zum Erstellen der Beispieltabellen dargestellt.
In den Befehlen werden die einzelnen Spalten der Tabelle mit ihren Datentypen angelegt.
Verschiedene \acs{SQL}-Erweiterungen unterstützen verschiedene Datentypen, mit denen Daten teilweise effizienter gespeichert werden können.
So bietet die \acs{SQL}-Erweiterung Transact-\acs{SQL} von Microsoft noch den Datentyp TINYINT der einen Wert von 0 bis 255 annehmen kann~\cite{ray_et_al_int_2023} und für die Spalte Quartal gut genutzt werden könnte.
PostgreSQL bietet hingegen keinen solchen Datentypen~\cite{the_postgresql_global_development_group_81_2023} und da im Verlauf dieser Arbeit mit PostgreSQL gearbeitet wurde, wird auch in diesem Beispiel kein TINYINT verwendet.

% Irgendwo noch OLAP CUBE beschreiben
\subsection{\acf{OLAP}}
\acf{OLAP}

















\subsection{Key Value Stores in \acs{OLAP}}



\section{\acf{SSB}}
Datenbank-Benchmarks geben einen Überblick über die Leistung von verschiedenen Datenbanksysteme und die Hardware, auf der diese laufen.
%Datenbank-Benchmark oder Datenbankbenchmark ?
Die Non-Profit-Organisation \acf{TPC} hat verschiedene Datenbank-Benchmarks entwickelt, die bei der Wahl des Datenbanksystems oder der Hardware helfen können~\cite[s. 619]{barata_overview_2015}.

Der TPC-H Benchmark wurde zum Testen von analytischen Datenbanken entwickelt.
Die Daten und Abfragen des Benchmarks sind auf eine Datenbank ausgelegt, mit der ein Anbieter seine Geschäftsdaten analysieren kann~\cite[S.~624-626]{barata_overview_2015}



\section{Redis}