\chapter{Implementierung}
Für diese Arbeit wurde der \acl{SSB} sowohl in PostgreSQL als auch in einer angepassten Form in Redis durchgeführt.
\section{\acl{SSB} in PostgreSQL}
\subsection{Generierung der \acs{SSB}-Daten}

\subsection{Ausführen des \acl{SSB} in PostgreSQL}

% Zeiten anpassen
Es existiert ein GitHub-Repository ~\cite{nukoyokohama_ssb-postgres_2023}, das eine Anleitung und Skripte für das Ausführen des \ac{SSB} in PostgreSQL bereitstellt.
Zur Ausführung wurde ein Docker-Container mit dem Image \enquote{postgres-standard} erstellt.
Dabei wurden die Standardwerte der PostgreSQL-Konfiguration verwendet.
Anschließend wurden die .tbl-Dateien und Skripte des Repositories in diesen Container geladen. % Deutschen Begriff für gebinded finden
Nach der Erstellung des Containers wird zunächst das Skript 'tables.sql' ausgeführt, um die Tabellen in der Datenbank anzulegen.
Danach können die Daten in die Tabellen importiert werden, indem das Skript 'load.sql' ausgeführt wird.
Abschließend kann der Benchmark durch die Ausführung des Skripts 'explain-analyze.sql' aus dem Repository durchgeführt werden.
% Hier ganze Anleitung im Anhang verlinken

% TODO: Ansatz aus Paper beleuchten
\section{\acs{SSB} in Redis}
Im Gegensatz zu relationalen Datenbanken ist das Konzept der Tabelle in Redis unbekannt.
Daten können daher nicht auf verschiedene Tabellen verteilt werden.
Es ist möglich, mehrere Datenbanken in Redis zu verwenden. Viele Erweiterungen bieten jedoch nur Unterstützung für die erste Datenbank.
Die Verwendung von mehreren Datenbanken in Redis wird als Anti-Pattern betrachtet und ist daher nicht empfohlen~\cite{prickett_answer_2022}.
Stattdessen werden künstliche Namensräume eingeführt, indem Präfixe zu Schlüsseln hinzugefügt werden.
Die Daten können damit in Kategorien eingeteilt werden.
Jede Zeile der .tbl-Dateien entspricht einer Zeile in einer der SQL-Tabellen.
Jede Zeile wurde unter einem einzigartigen Schlüssel mit dem Datentyp HASH in Redis gespeichert.

Im Rahmen dieser Forschung wurden verschiedene Ansätze zur Speicherung von Daten in Redis gewählt.




\subsection{RediSearch für Abfragen}
% Beschreiben, warum Redisearch zwingend notwendig ist

% Ab hier alles mit deepl write überarbeiten

% Darauf achten, dass Listing an richtiger Stelle ist
% TODO: Redis Pipeline in Grundlagen erklären und hie rverlinken

\subsubsection{Denormalisierter Ansatz}
Der Ansatz mit der tabellarischen Struktur erfordert das Verbinden von Pseudotabellen (Joins).
Es sind mehrere Abfragen erforderlich, um die Daten entweder clientseitig oder über ein LUA-Skript filtern zu können.
Dieser Umstand macht diesen Ansatz ineffizient und es ist unmöglich, die Berechnungen vollständig auf Seiten der Datenbank auszuführen.

Im Gegensatz zum tabellarischen Ansatz steht der denormalisierte Ansatz.

Um Joins vollständig zu vermeiden und eine vollständige clientseitige Verarbeitung zu ermöglichen, werden die Daten vollständig denormalisiert.
Die Tabellen werden aufgelöst
und alle Dimensionstabellen (DATE, PART, SUPPLIER, CUSTOMER) in die Faktentabelle (LINEORDER) integriert.

\subsubsection{Denormalisieren der Daten und Einfügen in Redis}
Um die Daten zu denormalisieren und in Redis zu speichern, wurde das Scala-Programm angepasst.

Das Programm definiert eigene Datentypen für die Daten der verschiedenen Tabellen (ShortenedLineorderObject, ShortenedCustomerObject, ShortenedSupplierObject, ShortenedPartObject, ShortenedDateObject).
Diese Datentypen enthalten Strings für alle Spalten der ursprünglichen Tabellen, die in den \acs{SSB}-Queries verwendet werden.
Des Weiteren wird im Zuge der Implementierung ein neuer Datentyp namens "DenormalizedObject" definiert,
der die Attribute anderer Datentypen in sich vereint und somit die denormalisierten Daten enthält.

Das Programm liest zunächst alle .tbl-Dateien ein, erzeugt daraus Objekte mit den zuvor definierten Datentypen und speichert diese als Werte in einer Map ab. Als Schlüssel dient dabei der Primärschlüssel in Form eines Strings.

Anschließend erfolgt eine Iteration über sämtliche "LINEORDER"-Einträge.
Für jedes Fremdschlüssel-Attribut wird das zugehörige Objekt aus der entsprechenden Map abgerufen.
Wenn also ein ShortenedLineorderObject beispielsweise den Wert X für suppkey hat, wird das ShortenedSupplierObject mit dem Schlüssel X aus der ShortenedSupplierObject-Map geladen.

Sobald alle Fremdschlüssel aufgelöst sind, wird ein DenormalizedObject erstellt und zusammen mit den Primärschlüsseln des ShortenedLineorderObject als Schlüssel in Redis gespeichert. (siehe \cref{code:ssb-redis-insert-denormalized-snippet}).


\begin{lstlisting}[
    language=scala,
    float,
    caption=Einfügen der denormalisierten Daten in Redis (Auszug),
    label=code:ssb-redis-insert-denormalized-snippet
]
private def createDenormalizedMap(
    customerMap: Map[String, ShortenedCustomerObject],
    supplierMap: Map[String, ShortenedSupplierObject],
    partMap: Map[String, ShortenedPartObject],
    lineorderMap: Map[String, ShortenedLineorderObject],
    dateMap: Map[String, ShortenedDateObject]
): Unit = {

  lineorderMap.foreach { case (key, lineorder) =>
    val customer = 
      customerMap.getOrElse(lineorder.lo_custkey, ShortenedCustomerObject("", "", "", ""))
    val supplier = 
      supplierMap.getOrElse(lineorder.lo_suppkey, ShortenedSupplierObject("", "", "", ""))
    val part = 
      partMap.getOrElse(lineorder.lo_partkey, ShortenedPartObject("", "", "", ""))
    val date = 
      dateMap.getOrElse(lineorder.lo_orderdate, ShortenedDateObject("", ""))

    val denormalized = DenormalizedObject(
      lineorder.lo_orderkey,
      ...
      customer.c_city,
      customer.c_nation,
      ...
    )

    denormalizedMap += (key -> denormalized)
  }
}

private def insertDenormalizedObjectsIntoRedis(pipeline: Pipeline): Unit = {
  denormalizedMap.values.grouped(1000).foreach { denormalizedObjects =>
    denormalizedObjects.foreach { denormalizedObject =>
      val hash = Map(
        "lo_orderkey" -> denormalizedObject.lo_orderkey,
        ...
        "c_city" -> denormalizedObject.c_city,
        "c_nation" -> denormalizedObject.c_nation,
        ...
      )

      pipeline.hmset("lineorder:" + denormalizedObject.lo_orderkey + ":" + denormalizedObject.lo_linenumber, hash.asJava)
      numOfInsertedRecords = numOfInsertedRecords + 1
    }
    pipeline.sync()
  }
}


\end{lstlisting}

% Darauf hinweisen, dass hier natürlich Daten verlorengehen, da nur für SSB Queries relevante Daten gespeichert werden

\subsubsection{Anpassen der Queries}
Mit RediSearch lassen sich die SQL-Queries des \acf{SSB} sehr gut für Redis anpassen.
\section{Praktische Implementierung}

\subsection{Scala}

\subsection{Scala mit Lua}

\section{Limitierungen der Implementierung}
